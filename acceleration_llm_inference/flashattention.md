# FlashAttention

## overview
FlashAttention reduces memory access to accelerate LLM.